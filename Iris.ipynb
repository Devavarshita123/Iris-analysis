#Check the versions of libraries 
#To know python version
import sys 
print('python: {0}'.format(sys.version))

#To know scipy version
#scipy is a free open source python library built on top of numpy, a wide array of tools for scientific and mathematical computations.
import scipy
print('scipy :{0}'.format(scipy.__version__))

#To know numpy version
#numpy : adding support for large, multi dimensional arrays and matrices along with a large collection of high level mathematical functions.
import numpy
print('numpy :{0}'.format(numpy.__version__))

#To know version of matplot library
# designed to help users visualize data in a variety of formats.
import matplotlib
print('matplotlib:{0}'.format(matplotlib.__version__))

#To know pandas version
#pandas it used for data manipulation and analysis consists of data structures and functions to perform efficient operations on data.
import pandas 
print('pandas:{0}'.format(pandas.__version__))

 #To know version of scikit learn
#scikit learn provides a simple and efficient way to perform various machine learning task
import sklearn
print('sklearn:{0}'.format(sklearn.__version__))

from pandas.plotting import scatter_matrix
import matplotlib.pyplot as plt
from sklearn import model_selection
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

#We can load the data from UCI Machine Learning Repository
#UCI-university of Calfornia, provides large array of datasets
url="https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data" 
#specify name of each column while loading the data
names=['sepal-length','sepal-width','petal-length','petal-width','class']
#We are using panda to load the dataset
dataset = pandas.read_csv(url,names=names)

print(dataset.shape)#number of rows and columns in a dataset

#print data of first 30 rows
print(dataset.head(30))

#To get al the mean,std,min
print(dataset.describe())

# to get number of record by the class tghe belong to
print(dataset.groupby('class').size())

#uni variant plot
#dataset.plot(): This is the function used to create the plot.
#kind='box': This specifies that a boxplot should be created. Boxplots are used to visualize the distribution of a dataset by displaying the median, quartiles, and outliers.
#subplots=True: This specifies that each column of the dataset should be plotted in a separate subplot.
#layout=(2,2): This specifies the layout of the subplots. In this case, the subplots will be arranged in a 2x2 grid.
#sharex=False and sharey=False: These specify that the x and y axes should not be shared across subplots. This means that each subplot will have its own x and y axis scales.
dataset.plot(kind='box',subplots = True,layout=(2,2),sharex = False,sharey = False)
plt.show()

dataset.hist()
plt.show()

#multi varaint plot
scatter_matrix(dataset)
plt.show()

#upto now we are know about our data sets and their attributes
#Now, we will create a validata data set(i.e, training data model) using algorithm and see the hidden analysis
# Now the data will be split into two parts, one to train the data and other to verify our trained model
#The .values will convert data frame into NumPy array
#NumPy array = A spreadsheet (you can do fast calculations, use rows & columns, apply formulas)
array=dataset.values#The data set is a pandas data frame
x = array[:,0:4]#selects first four column form 0 to 3 indices
#This selects the 5th column (index 4).
#This column is usually the target/output you want to predict.
#y is your labels array.
y = array[:,4]
validation_size = 0.20 # 20 % is used to test the model, remaining 80% to train the model
#This is used to set a fixed random pattern for splitting data.
#It makes sure that every time you run the code, the split is the same, for consistency.
seed = 6
#test_split() will split data into training and testing model
#x_train,x_test inputs for training and testing 
#y_test,y_train are the labels for testing and training
x_train,x_test,y_train,y_test  = model_selection.train_test_split(x,y,test_size= validation_size,random_state = seed)

seed = 6
#This tells your model evaluation method to measure performance using "accuracy", “How many predictions did the model get right out of all predictions?”
scoring  = 'accuracy'

#These performs cross-validation on my data,i.e, how well our model performs on unseen data
models = []
#Adding each model as a tuple, creating short names 'LR'
models.append(('LR', LogisticRegression(max_iter=1000)))
#With max_iter = 1000, it won't generate ,convergenceWarning from scikit-learn’s Logistic Regression model. It means that the optimization algorithm (lbfgs) couldn't reach a solution within the default number of iterations.
models.append(('LDA', LinearDiscriminantAnalysis()))
models.append(('KNN', KNeighborsClassifier()))
models.append(('CART', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
results = []
names = []
#Goes through each model one by one
for name, model in models:
    #This splits your training data into 10 parts.
    #In each round, it trains on 9 parts and tests on 1 part.
    #It does this 10 times, each time using a different part as the test set.
    #shuffle=True: Mix the data before splitting.
    #random_state=seed: Ensures reproducibility (same split every tim
    kfold = model_selection.KFold(n_splits=10,shuffle = True, random_state=seed)
    #This performs the actual training and testing.
    #It returns 10 accuracy scores (one for each split).
    #x_train, y_train: Your training features and labels.
    #scoring='accuracy': The metric used to score performance.
    cv_results = model_selection.cross_val_score(model, x_train, y_train, cv=kfold, scoring=scoring)
    results.append(cv_results)
    names.append(name)
    msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
    print(msg)
